---
title: 'SLiM-Gym: Reinforcement Learning Environments for Population Genetics'
tags:
  - Population genetics
  - Reinforcement learning
  - Simulation
  - Python
authors:
  - name: Niko Zuppas
    corresponding: true
    orcid: 0000-0000-0000-0000
    equal-contrib: true
    affiliation: 1
  - name: Bryan C. Carstens
    orcid: 0000-0000-0000-0000
    equal-contrib: true
    affiliation: 1
affiliations:
 - name: Department of Evolution, Ecology, and Organismal Biology and Museum of Biological Diversity. 1315 Kinnear Rd., Columbus OH 43212
   index: 1
date: 21 April 2025
bibliography: paper.bib

# Summary
Wright-Fisher evolutionary dynamics provide a mathematical framework for modeling populations over discrete time steps [1]. Deep reinforcement learning (RL) has proven highly effective in optimizing complex sequential decisions, achieving expert-level performance in domains like Poker and Go [2, 3]. However, applying RL to evolutionary problems requires suitable training environments. We present SLiM-Gym, a Python package that bridges this gap by connecting the Gymnasium RL framework with SLiM, a forward-time population genetics simulator, enabling researchers to apply RL methods to study evolutionary processes and generate novel hypotheses.

# Statement of Need
Evolutionary trajectories are shaped by the interaction of genetic forces acting sequentially across generations. While theoretical models like Wright-Fisher capture these dynamics with well-defined transition probabilities, unified tools for studying and controlling evolutionary processes through reinforcement learning have been limited. Recent work demonstrated the potential of reinforcement learning for controlling ecological and physiological aspects of evolution [4], optimizing breeding programs [5] and addressing emergent drug resistance [6]. However, these implementations rely on custom simulation tools, potentially limiting accessibility for population geneticists. Reinforcement learning has also been applied to phylogenetic tree space [7] and ancestral recombination graph construction [8], highlighting the broader potential of RL in evolutionary biology.

SLiM 4 [9] has emerged as a popular forward-time population genetics simulator, with extensive adoption among population geneticists. It excels at modeling complex evolutionary scenarios with individual-level resolution and supports extensive customization through its Eidos scripting language. However, it lacks agentic optimization or exploration. Gymnasium [10], originally released as Gym by OpenAI in 2016 [11], provides a standardized framework for RL research, while environments like MuJoCo, an advanced physics simulator [12],  illustrate the value of coupling RL with high-fidelity simulations. SLiM-Gym combines SLiM's evolutionary modeling with Gymnasium's RL framework, allowing us to leverage RL to generate testable hypotheses about evolutionary processes and outcomes.

# SLiM-Gym Wrapper
The base wrapper extends Gymnasium with modifications for SLiM integration. When initializing a new training episode, the wrapper launches SLiM as a subprocess to run a user-defined simulation recipe as an agentic environment. Communication between Gymnasium and SLiM occurs through a file-based protocol, enabling real-time manipulation of simulation parameters. The wrapper provides abstract methods that environments must implement to handle state processing, action translation, and reward calculation.

# Communication Protocol
The SLiM-Gym communication protocol begins by starting the SLiM subprocess and initializing a signaling file. During initialization and burn-in generations, SLiM operates independently of Python. Once this phase is complete, synchronization between SLiM and SLiM-Gym is achieved through two mechanisms: SLiM’s output logging and the signaling file. After burn-in, SLiM logs generation data and deletes the signaling file, entering a waiting state. The wrapper’s step function monitors for both presence of the log file and absence of the signaling file, proceeding to execute when both conditions are met. The action generated by the learning algorithm is written to a new signaling file, lifting SLiM’s waiting condition and allowing the action to be applied to the simulation before continuing. The process is visualized in Figure 1.






